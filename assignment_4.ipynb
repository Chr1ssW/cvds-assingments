{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "NoneType = type(None)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "\n",
    "# You can copy this code to your personal pipeline project or execute it here.\n",
    "def train_gan(batch_size: int = 32, num_epochs: int = 100, device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    The method trains a Generative Adversarial Network and is based on:\n",
    "    https://realpython.com/generative-adversarial-networks/\n",
    "\n",
    "    The Generator network tries to generate convincing images of handwritten digits.\n",
    "    The Discriminator needs to detect if the image was created by the Generater or if the image is a real image from\n",
    "    a known dataset (MNIST).\n",
    "    If both the Generator and the Discriminator are optimized, the Generator is able to create images that are difficult\n",
    "    to distinguish from real images. This is goal of a GAN.\n",
    "\n",
    "    This code produces the expected results at first attempt at about 50 epochs.\n",
    "\n",
    "    :param batch_size: The number of images to train in one epoch.\n",
    "    :param num_epochs: The number of epochs to train the gan.\n",
    "    :param device: The computing device to use. If CUDA is installed and working then `cuda:0` is chosen\n",
    "        otherwise 'cpu' is chosen. Note: Training a GAN on the CPU is very slow.\n",
    "\n",
    "    **This method is part of a series of debugging exercises.**\n",
    "    **Each Python method of this series contains bug that needs to be found.**\n",
    "\n",
    "    It contains at least two bugs: one structural bug and one cosmetic bug. Both bugs are from the original tutorial.\n",
    "\n",
    "    | ``1   Changing the batch_size from 32 to 64 triggers the structural bug.``\n",
    "    | ``2   Can you also spot the cosmetic bug?``\n",
    "    | ``Note: to fix this bug a thorough understanding of GANs is not necessary.``\n",
    "\n",
    "    Change the batch size to 64 to trigger the bug with message:\n",
    "    ValueError: \"Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([96, 1])) is deprecated. Please ensure they have the same size.\"\n",
    "\n",
    "    >>> train_gan(batch_size=32, num_epochs=100)\n",
    "    \"\"\"\n",
    "    # Add/adjust code.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    try:\n",
    "        train_set = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "    except:\n",
    "        print(\"Failed to download MNIST, retrying with different URL\")\n",
    "        # see: https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py\n",
    "        torchvision.datasets.MNIST.resources = [\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz',\n",
    "             'f68b3c2dcbeaaa9fbdd348bbdeb94873'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'd53e105ee54ea40749a09fcbcd1e9432'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             '9fb629c4189551a2d022fa330f9573f3'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "             'ec29112dd5afa0611ce80d1b7f02629c')\n",
    "        ]\n",
    "        train_set = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # example data\n",
    "    real_samples, mnist_labels = next(iter(train_loader))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(16):\n",
    "        sub = fig.add_subplot(4, 4, 1 + i)\n",
    "        sub.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "        sub.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(\"Real images\")\n",
    "    display(fig)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Set up training\n",
    "    discriminator = Discriminator().to(device)\n",
    "    generator = Generator().to(device)\n",
    "    lr = 0.0001\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(num_epochs):\n",
    "        for n, (real_samples, mnist_labels) in enumerate(train_loader):\n",
    "            actual_batch_size = real_samples.size(0)\n",
    "\n",
    "            # Data for training the discriminator\n",
    "            real_samples = real_samples.to(device=device)\n",
    "            real_samples_labels = torch.ones((actual_batch_size, 1)).to(device=device)\n",
    "            latent_space_samples = torch.randn((actual_batch_size, 100)).to(device=device)\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            generated_samples_labels = torch.zeros((actual_batch_size, 1)).to(device=device)\n",
    "            all_samples = torch.cat((real_samples, generated_samples))\n",
    "            all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "            # Training the discriminator\n",
    "            discriminator.zero_grad()\n",
    "            output_discriminator = discriminator(all_samples)\n",
    "            loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # Data for training the generator\n",
    "            latent_space_samples = torch.randn((actual_batch_size, 100)).to(device=device)\n",
    "\n",
    "            # Training the generator\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            loss_generator = loss_function(output_discriminator_generated, real_samples_labels)\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # Show loss and samples generated\n",
    "            if n == batch_size - 1:\n",
    "                name = f\"Generate images\\n Epoch: {epoch} Loss D.: {loss_discriminator:.2f} Loss G.: {loss_generator:.2f}\"\n",
    "                generated_samples = generated_samples.detach().cpu().numpy()\n",
    "                fig = plt.figure()\n",
    "                for i in range(16):\n",
    "                    sub = fig.add_subplot(4, 4, 1 + i)\n",
    "                    sub.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "                    sub.axis('off')\n",
    "                fig.suptitle(name)\n",
    "                fig.tight_layout()\n",
    "                clear_output(wait=False)\n",
    "                display(fig)\n",
    "\n",
    "# You can copy this code to your personal pipeline project or execute it here.\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class for the GAN\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 784)\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "# You can copy this code to your personal pipeline project or execute it here.\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator class for the GAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        output = output.view(x.size(0), 1, 28, 28)\n",
    "        return output\n",
    "\n",
    "train_gan(batch_size=64, num_epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
